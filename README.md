# IntelliDocs

**IntelliDocs** is an end-to-end AI-powered document exploration and summarization platform. It uses a **Retrieval-Augmented Generation (RAG)** approach with Large Language Models (LLMs) to help you ask questions about PDF documents and receive contextually grounded, concise answers. 

## Table of Contents

- [Features](#features)  
- [Architecture](#architecture)  
- [Installation](#installation)  
  - [Prerequisites](#prerequisites)  
  - [Setup Steps](#setup-steps)  
- [Usage](#usage)  
  - [Local Execution](#local-execution)  
  - [Docker Execution](#docker-execution)  
- [Project Structure](#project-structure)  
- [How It Works](#how-it-works)  
  - [Document Processing and Chunking](#document-processing-and-chunking)  
  - [Embeddings and Vector Storage](#embeddings-and-vector-storage)  
  - [Query & Answer Generation](#query--answer-generation)  
- [Customization](#customization)  
- [Roadmap](#roadmap)  
- [License](#license)  

---

## Features

- **Simple Upload & Query**: Drag-and-drop PDFs into a Streamlit UI and start asking questions.  
- **Efficient Text Chunking**: Splits long PDFs into smaller, manageable sections.  
- **Semantic Search**: Stores chunk embeddings in a vector database for quick, relevant retrieval.  
- **LLM-Powered Answers**: Answers are generated by state-of-the-art language models, grounded in retrieved chunks to reduce hallucinations.  
- **Modular & Scalable**: Easily swap out components like the vector database (FAISS, Milvus, Chroma) or the LLM (DeepSeek-R1:1.5b, Ollama, OpenAI, etc.).  
- **Developer-Friendly**: Clear folder structure and code organization to encourage experimentation and extension.

---

## Architecture

```
               ┌─────────────────────┐
               │     User Browser    │
               │ (Streamlit Frontend)│
               └─────────┬───────────┘
                         │
                         │
                         ▼
               ┌─────────────────────┐
               │      IntelliDocs    │
               │     (app.py)        │
               └─────────┬───────────┘
                         │
                         ▼
        ┌────────────────────────────────────┐
        │ RAG Pipeline: Chunk -> Embed ->   │
        │  Store -> Retrieve -> Generate    │
        └───────────┬───────────────────────┘
                    │
                    ▼
    ┌────────────────────────────────────────┐
    │   Vector Database (e.g., FAISS)       │
    └────────────────────────────────────────┘
```

---

## Installation

### Prerequisites

- Python 3.8 or higher
- [pip](https://pypi.org/project/pip/)
- (Optional) [Docker](https://www.docker.com/) for containerized deployments

### Setup Steps

1. **Clone this repository**:
   ```bash
   git clone https://github.com/yourusername/IntelliDocs.git
   cd IntelliDocs
   ```

2. **Install Python dependencies**:
   ```bash
   pip install -r requirements.txt
   ```
   > Note: If you plan to use cloud-based LLM or embedding services (like OpenAI), ensure you add and configure those dependencies (e.g., `openai`).

3. **Configure environment variables** (if using API-based embeddings or LLMs). For example:
   ```bash
   export OPENAI_API_KEY="your_openai_key_here"
   ```
   Or create a `.env` file and load it in your code as needed.

---

## Usage

### Local Execution

1. **Start the Streamlit app**:
   ```bash
   streamlit run app.py
   ```
2. **Open your browser** at the URL provided in the console (usually [http://localhost:8501](http://localhost:8501)).
3. **Upload a PDF** file. Once the document finishes processing, type your questions into the text input box.

### Docker Execution

1. **Build the Docker image**:
   ```bash
   docker build -t intellidocs:latest .
   ```
2. **Run the Docker container**:
   ```bash
   docker run -p 8501:8501 intellidocs:latest
   ```
3. **Access** at [http://localhost:8501](http://localhost:8501) on your host machine.

---

## Project Structure

```
IntelliDocs/
├── README.md               # You're here!
├── requirements.txt
├── Dockerfile              # For optional containerization
├── app.py                  # Streamlit entry point
|
├── src/
│   ├── chunking.py         # PDF parsing & text chunking
│   ├── embeddings.py       # Generating & managing vector embeddings
│   ├── retrieval.py        # Searching the vector store
│   ├── rag_pipeline.py     # High-level RAG orchestration
│   └── llm_inference.py    # Functions for local/cloud LLM inference
|
├── data/
│   └── sample.pdf          # Example PDF for testing
|
└── docs/
    └── architecture.png    # Architecture diagrams or references
```

---

## How It Works

### Document Processing and Chunking

1. **Extract Text**: The PDF is parsed using [pdfplumber](https://pypi.org/project/pdfplumber/) or [PyPDF2](https://pypi.org/project/PyPDF2/).  
2. **Chunking**: Large text is split into smaller segments for efficient embedding and retrieval (usually around 500 tokens or words per chunk).

### Embeddings and Vector Storage

- Each chunk is converted into a *vector embedding* using a model of your choice (e.g., OpenAI’s `text-embedding-ada-002` or a local model).  
- Embeddings are stored in a **vector database** (e.g., FAISS). At query time, the system performs a nearest-neighbor search to quickly find the most relevant chunks.

### Query & Answer Generation

1. **User Query**: Converts the user’s question into an embedding.  
2. **Nearest Neighbors**: Retrieves the top-k most relevant chunks.  
3. **Answer Composition**: Passes those chunks as context to an LLM (e.g., GPT-3.5, a local model, etc.). The LLM then generates a concise, context-aware response.

---

## Customization

1. **Local vs. Cloud Models**: Swap out OpenAI embeddings for a local embedding model, or switch the LLM from GPT-3.5 to another provider.  
2. **Vector Database**: Choose from [FAISS](https://github.com/facebookresearch/faiss), [Milvus](https://milvus.io/), or [Chroma](https://www.trychroma.com/).  
3. **UI Adjustments**: Tailor the Streamlit interface to your workflow (e.g., add fields for specifying the number of chunks to retrieve, or an option to produce longer or shorter summaries).  
4. **Advanced NLP**: Implement additional features like Named Entity Recognition (NER), sentiment analysis, or PDF metadata extraction.

---

## Roadmap

- **Multi-Document Management**: Index multiple PDFs simultaneously for cross-document queries.  
- **OCR Support**: Integrate an OCR pipeline (e.g., Tesseract) for scanned or image-based PDFs.  
- **Fine-Tuning**: Explore domain-specific fine-tuning for specialized use cases (e.g., legal or medical texts).  
- **Advanced Caching**: Store partial query results to boost performance for repeated queries.  
- **User Authentication**: Add secure user management for private or sensitive document handling.

---

## License

[MIT License](LICENSE) – Feel free to modify, distribute, and build upon this project for both commercial and non-commercial purposes. See [LICENSE](LICENSE) for details.

---

### Enjoy IntelliDocs!

For any questions or contributions, please open an Issue or Pull Request. We welcome feedback and collaboration to make IntelliDocs even more powerful and developer-friendly.
